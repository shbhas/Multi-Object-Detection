{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import importlib\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "#MY_DIRNAME = os.path.dirname(os.path.abspath(states))\n",
    "#sys.path.insert(0, os.path.join(MY_DIRNAME, '..'))\n",
    "\n",
    "# TODO: import net \n",
    "from yolo_model import yoloModel\n",
    "from PASCAL_Dataloader import create_split_loaders\n",
    "from YOLO_Loss import YoloLoss\n",
    "\n",
    "total_loss = defaultdict(dict)\n",
    "avg_minibatch_loss = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    config['global_step'] = config.get('start_step', 0)\n",
    "    is_training = False if config.get('export_onnx') else True\n",
    "\n",
    "    # TODO: Load and initialize network\n",
    "    net = yoloModel(config)\n",
    "\n",
    "    # Define the optimizer and learning rate\n",
    "    optimizer = obtain_optimizer(config, net)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "        step_size=config['decay_step'],\n",
    "        gamma=config['decay_gamma'])\n",
    "\n",
    "    # Use pretrained model\n",
    "    if config['pretrain_snapshot']:\n",
    "        print('Hello')\n",
    "        logging.info('Load pretrained weights from {}'.format(config['pretrain_snapshot']))\n",
    "        state_dict = torch.load(config['pretrain_snapshot'])\n",
    "        net.load_state_dict(state_dict)\n",
    "\n",
    "    # Use all 3 scales for computing YOLO loss\n",
    "    YOLO_losses = []\n",
    "    for i in range(3):\n",
    "        YOLO_losses.append(YoloLoss(config['classes'], (config['img_w'], config['img_h']), config['anchors'][i]))\n",
    "\n",
    "    # Check if your system supports CUDA\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Setup GPU optimization if CUDA is supported\n",
    "    if use_cuda:\n",
    "        computing_device = torch.device(\"cuda\")\n",
    "        extras = {\"num_workers\": 3, \"pin_memory\": True}\n",
    "        print(\"CUDA is supported\")\n",
    "    else: # Otherwise, train on the CPU\n",
    "        computing_device = torch.device(\"cpu\")\n",
    "        extras = False\n",
    "        print(\"CUDA NOT supported\")\n",
    "    \n",
    "    # Load in data \n",
    "    root_dir = os.getcwd()\n",
    "    train_loader, val_loader, test_loader = create_split_loaders(root_dir, config['batch_size'])\n",
    "    \n",
    "    # Instantiate model to run on the GPU or CPU based on CUDA support\n",
    "    net = net.to(computing_device)\n",
    "    print(\"Model on CUDA?\", next(net.parameters()).is_cuda)\n",
    "    \n",
    "    # Begin training loop\n",
    "    print(\"Start training:\")\n",
    "    for epoch in range(config['epochs']):\n",
    "        N_minibatch_loss = 0.0\n",
    "        for minibatch, samples in enumerate(train_loader):\n",
    "            images, labels = samples[\"image\"], samples[\"label\"]\n",
    "            start_time = time.time()\n",
    "            config['global_step'] += 1\n",
    "                \n",
    "            #images = images.unsqueeze(0)\n",
    "\n",
    "            # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "            images = images.to(computing_device)\n",
    "            #labels = labels.to(computing_device)\n",
    "\n",
    "            # Forward and backward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss_names = [\"total_loss\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"cls\"]\n",
    "            losses = []\n",
    "            for z in range(len(loss_names)):\n",
    "                losses.append([])\n",
    "            for i in range(3):\n",
    "                loss_item = YOLO_losses[i](outputs[i], labels)\n",
    "                for j, l in enumerate(loss_item):\n",
    "                    losses[j].append(l)\n",
    "            losses = [sum(l) for l in losses]\n",
    "            loss = losses[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add this iteration's loss to the total_loss\n",
    "            total_loss[epoch][minibatch] = loss.item()\n",
    "            N_minibatch_loss += loss\n",
    "\n",
    "            if minibatch > 0 and minibatch % 10 == 0:\n",
    "                _loss = loss.item()\n",
    "                N_minibatch_loss /= 10\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                print('Epoch [%.3d] Minibatch = %d Loss = %.2f lr = %.5f '%\n",
    "                    (epoch, minibatch, N_minibatch_loss, lr))\n",
    "                \n",
    "                # Add the averaged loss over N minibatches and reset the counter\n",
    "                avg_minibatch_loss[epoch][minibatch] = N_minibatch_loss.item()\n",
    "                N_minibatch_loss = 0.0\n",
    "                \n",
    "                config['tensorboard_writer'].add_scalar(\"lr\",\n",
    "                                                        lr,\n",
    "                                                        config['global_step'])\n",
    "                for i, name in enumerate(loss_names):\n",
    "                    value = _loss if i == 0 else losses[i]\n",
    "                    config['tensorboard_writer'].add_scalar(name,\n",
    "                                                            value,\n",
    "                                                            config['global_step'])\n",
    "\n",
    "            if minibatch > 0 and minibatch % 100 == 0:\n",
    "                save_checkpoint(net.state_dict(), config)\n",
    "                \n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    save_checkpoint(net.state_dict(), config)\n",
    "    print('Training Complete')\n",
    "\n",
    "    \n",
    "def save_checkpoint(state_dict, config, evaluate_func=None):\n",
    "        \n",
    "    checkpoint_path = os.path.join(config[\"sub_working_dir\"], \"model.pth\")\n",
    "    torch.save(state_dict, checkpoint_path)\n",
    "    print(\"Model checkpoint saved to %s\" % checkpoint_path)\n",
    "\n",
    "\n",
    "def obtain_optimizer(config, net):\n",
    "    optimizer = None\n",
    "\n",
    "    # Assign different learning rate for each layer\n",
    "    params = None\n",
    "    base_parameters = list(\n",
    "        map(id, net.backbone.parameters())\n",
    "    )\n",
    "    logits_parameters = filter(lambda p: id(p) not in base_parameters, net.parameters())\n",
    "\n",
    "    if not config['freeze_backbone']:\n",
    "        parameters = [\n",
    "            {\"params\": logits_parameters, \"lr\": config['other_lr']},\n",
    "            {\"params\": net.backbone.parameters(), \"lr\": config['backbone_lr']},\n",
    "        ]\n",
    "    else:\n",
    "        print(\"Freezing backbone parameters\")\n",
    "        for p in net.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        parameters = [\n",
    "            {\"params\": logits_parameters, \"lr\": config['other_lr']},\n",
    "        ]\n",
    "\n",
    "    # Initialize optimizer class\n",
    "    if config['optimizer_type'] == \"adam\":\n",
    "        optimizer = optim.Adam(params, weight_decay=config['optimizer_weight_decay'])\n",
    "    elif config['optimizer_type'] == \"amsgrad\":\n",
    "        optimizer = optim.Adam(params, weight_decay=config['optimizer_weight_decay'], amsgrad=True)\n",
    "    elif config['optimizer_type'] == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(params, weight_decay=config['optimizer_weight_decay'])\n",
    "    else:\n",
    "        optimizer = optim.SGD(parameters, momentum=0.9, weight_decay=config['optimizer_weight_decay'],\n",
    "                              nesterov=(config['optimizer_type'] == \"nesterov\"))\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Initialize hyperparameters/variables\n",
    "    config = {}\n",
    "    config['backbone_name'] = \"darknet_53\"\n",
    "    config['backbone_pretrained'] = \"./darknet53_weights_pytorch.pth\" # set empty to disable\n",
    "    \n",
    "    config['anchors'] = [[[116, 90], [156, 198], [373, 326]],\n",
    "                                [[30, 61], [62, 45], [59, 119]],\n",
    "                                [[10, 13], [16, 30], [33, 23]]]\n",
    "    config['classes'] = 20\n",
    "    \n",
    "    config['backbone_lr'] = 0.001\n",
    "    config['other_lr'] = 0.01\n",
    "    config['freeze_backbone'] = False   #  freeze backbone wegiths to finetune\n",
    "    config['decay_gamma'] = 0.1\n",
    "    config['decay_step'] = 10         #  decay lr in every ? epochs\n",
    "    \n",
    "    config['optimizer_type'] = \"sgd\"\n",
    "    config['optimizer_weight_decay'] = 4e-05\n",
    "    \n",
    "    config['batch_size'] = 16  # Number of training samples per batch to be passed to network\n",
    "    config['epochs'] = 20  # Number of epochs to train the model\n",
    "    config['img_h'] = config['img_w'] = 416\n",
    "    config['seed'] = np.random.seed()\n",
    "    config['working_dir'] = \"./states\"     #  replace with your working dir\n",
    "    \n",
    "    def get_latest_states(dirpath):\n",
    "        \"\"\"\n",
    "        Get the latest image file in the given directory\n",
    "        \"\"\"\n",
    "        # get filepaths of all files and dirs in the given dir\n",
    "        valid_files = [os.path.join(dirpath, filename) for filename in os.listdir(dirpath)]\n",
    "\n",
    "        return max(valid_files, key=os.path.getmtime)\n",
    "   \n",
    "    \n",
    "    # Create sub_working_dir\n",
    "    sub_working_dir = '{}/{}'.format(\n",
    "        config['working_dir'], time.strftime(\"%Y%m%d%H%M%S\", time.localtime()))\n",
    "    #if not os.path.exists(sub_working_dir):\n",
    "        #os.makedirs(sub_working_dir)\n",
    "    config[\"sub_working_dir\"] = sub_working_dir\n",
    "    logging.info(\"sub working dir: %s\" % sub_working_dir)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(sub_working_dir, \"model.pth\")):\n",
    "        config['pretrain_snapshot'] = \"\"\n",
    "    else:\n",
    "        config['pretrain_snapshot'] = os.path.join(get_latest_states(config['working_dir']), \"model.pth\")       #  load checkpoint\n",
    "        \n",
    "    # Create tf_summary writer\n",
    "    config[\"tensorboard_writer\"] = SummaryWriter(sub_working_dir)\n",
    "\n",
    "    # Start training\n",
    "    train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "Model on CUDA? True\n",
      "Start training:\n",
      "Epoch [000] Minibatch = 10 Loss = 3.16 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 20 Loss = 1.81 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 30 Loss = 1.27 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 40 Loss = 1.03 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 50 Loss = 0.90 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 60 Loss = 0.85 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 70 Loss = 0.76 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 80 Loss = 0.74 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 90 Loss = 0.68 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 100 Loss = 0.67 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 190 Loss = 0.54 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 200 Loss = 0.52 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [000] Minibatch = 210 Loss = 0.55 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 220 Loss = 0.52 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 230 Loss = 0.52 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 240 Loss = 0.51 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 250 Loss = 0.51 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 260 Loss = 0.51 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 270 Loss = 0.52 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 280 Loss = 0.52 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 290 Loss = 0.53 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 300 Loss = 0.51 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [000] Minibatch = 310 Loss = 0.51 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 320 Loss = 0.51 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 330 Loss = 0.51 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 340 Loss = 0.50 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 350 Loss = 0.50 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 360 Loss = 0.47 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 370 Loss = 0.55 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 380 Loss = 0.50 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 390 Loss = 0.50 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 400 Loss = 0.48 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [000] Minibatch = 410 Loss = 0.48 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 420 Loss = 0.44 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 430 Loss = 0.50 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 440 Loss = 0.46 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 450 Loss = 0.46 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 460 Loss = 0.46 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 470 Loss = 0.47 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 480 Loss = 0.47 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 490 Loss = 0.48 lr = 0.01000 \n",
      "Epoch [000] Minibatch = 500 Loss = 0.46 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [000] Minibatch = 510 Loss = 0.45 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 10 Loss = 0.51 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 20 Loss = 0.45 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 30 Loss = 0.45 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 40 Loss = 0.43 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 50 Loss = 0.46 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 60 Loss = 0.42 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 70 Loss = 0.40 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 80 Loss = 0.44 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 90 Loss = 0.45 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 100 Loss = 0.46 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [001] Minibatch = 110 Loss = 0.47 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 120 Loss = 0.43 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 130 Loss = 0.44 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 140 Loss = 0.44 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 150 Loss = 0.45 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 160 Loss = 0.46 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 170 Loss = 0.43 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 180 Loss = 0.42 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 190 Loss = 0.41 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 200 Loss = 0.44 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [001] Minibatch = 210 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 220 Loss = 0.44 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 230 Loss = 0.40 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 240 Loss = 0.42 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 250 Loss = 0.42 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 260 Loss = 0.41 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 270 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 280 Loss = 0.44 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 290 Loss = 0.40 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 300 Loss = 0.40 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [001] Minibatch = 310 Loss = 0.40 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 320 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 330 Loss = 0.41 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 340 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 350 Loss = 0.40 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 360 Loss = 0.40 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 370 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 380 Loss = 0.41 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 390 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 400 Loss = 0.39 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [001] Minibatch = 410 Loss = 0.41 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 420 Loss = 0.40 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 430 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 440 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 450 Loss = 0.41 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 460 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 470 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 480 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 490 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [001] Minibatch = 500 Loss = 0.34 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [001] Minibatch = 510 Loss = 0.41 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 10 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 20 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 30 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 40 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 50 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 60 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 70 Loss = 0.42 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 80 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 90 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 100 Loss = 0.37 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [002] Minibatch = 110 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 120 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 130 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 140 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 150 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 160 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 170 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 180 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 190 Loss = 0.39 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 200 Loss = 0.37 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [002] Minibatch = 210 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 220 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 230 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 240 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 250 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 260 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 270 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 280 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 290 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 300 Loss = 0.40 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [002] Minibatch = 310 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 320 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 330 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 340 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 350 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 360 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 370 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 380 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 390 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 400 Loss = 0.37 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [002] Minibatch = 410 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 420 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 430 Loss = 0.36 lr = 0.01000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [002] Minibatch = 440 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 450 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 460 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 470 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 480 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 490 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [002] Minibatch = 500 Loss = 0.38 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [002] Minibatch = 510 Loss = 0.38 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 10 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 20 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 30 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 40 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 50 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 60 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 70 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 80 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 90 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 100 Loss = 0.34 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [003] Minibatch = 110 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 120 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 130 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 140 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 150 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 160 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 170 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 180 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 190 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 200 Loss = 0.36 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [003] Minibatch = 210 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 220 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 230 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 240 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 250 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 260 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 270 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 280 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 290 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 300 Loss = 0.33 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [003] Minibatch = 310 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 320 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 330 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 340 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 350 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 360 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 370 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 380 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 390 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 400 Loss = 0.33 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [003] Minibatch = 410 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 420 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 430 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 440 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 450 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 460 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 470 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 480 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 490 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [003] Minibatch = 500 Loss = 0.36 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [003] Minibatch = 510 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 10 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 20 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 30 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 40 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 50 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 60 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 70 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 80 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 90 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 100 Loss = 0.31 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [004] Minibatch = 110 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 120 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 130 Loss = 0.36 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 140 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 150 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 160 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 170 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 180 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 190 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 200 Loss = 0.36 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [004] Minibatch = 210 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 220 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 230 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 240 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 250 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 260 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 270 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 280 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 290 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 300 Loss = 0.35 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [004] Minibatch = 310 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 320 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 330 Loss = 0.26 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 340 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 350 Loss = 0.37 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 360 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 370 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 380 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 390 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 400 Loss = 0.31 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [004] Minibatch = 410 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 420 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 430 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 440 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 450 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 460 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 470 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 480 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 490 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [004] Minibatch = 500 Loss = 0.33 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [004] Minibatch = 510 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 10 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 20 Loss = 0.27 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 30 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 40 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 50 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 60 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 70 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 80 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 90 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 100 Loss = 0.34 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [005] Minibatch = 110 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 120 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 130 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 140 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 150 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 160 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 170 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 180 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 190 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 200 Loss = 0.27 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [005] Minibatch = 210 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 220 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 230 Loss = 0.34 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 240 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 250 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 260 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 270 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 280 Loss = 0.29 lr = 0.01000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [005] Minibatch = 290 Loss = 0.27 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 300 Loss = 0.31 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [005] Minibatch = 310 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 320 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 330 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 340 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 350 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 360 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 370 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 380 Loss = 0.35 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 390 Loss = 0.31 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 400 Loss = 0.30 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [005] Minibatch = 410 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 420 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 430 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 440 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 450 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 460 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 470 Loss = 0.27 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 480 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 490 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [005] Minibatch = 500 Loss = 0.27 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [005] Minibatch = 510 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 10 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 20 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 30 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 40 Loss = 0.28 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 50 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 60 Loss = 0.27 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 70 Loss = 0.30 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 80 Loss = 0.32 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 90 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 100 Loss = 0.30 lr = 0.01000 \n",
      "Model checkpoint saved to ./states/20190523200450/model.pth\n",
      "Epoch [006] Minibatch = 110 Loss = 0.33 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 120 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 130 Loss = 0.26 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 140 Loss = 0.29 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 150 Loss = 0.27 lr = 0.01000 \n",
      "Epoch [006] Minibatch = 160 Loss = 0.29 lr = 0.01000 \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
