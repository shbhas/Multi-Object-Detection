{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import traceback\n",
    "import logging\n",
    "import importlib\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "\n",
    "from yolo_model import yoloModel\n",
    "from yolo_model_dcnn import dcnnyoloModel\n",
    "from PASCAL_Dataloader import create_split_loaders\n",
    "from YOLO_Loss import YoloLoss\n",
    "\n",
    "total_loss = defaultdict(dict)\n",
    "avg_minibatch_loss = []\n",
    "avg_valid_loss = []\n",
    "avg_test_loss = defaultdict(dict)\n",
    "_validation_metrics = defaultdict(dict)\n",
    "tot_test_outputs = []\n",
    "tot_test_labels = []\n",
    "\n",
    "log = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, architecture = 'Baseline'):\n",
    "    \n",
    "    config['global_step'] = config.get('start_step', 0)\n",
    "    is_training = False if config.get('export_onnx') else True\n",
    "\n",
    "    # Load and initialize network\n",
    "    if architecture == \"Baseline\":\n",
    "        net = yoloModel(config)\n",
    "    elif architecture == \"DeformConv\":\n",
    "        net = dcnnyoloModel(config)\n",
    "    else:\n",
    "        print('Error: Incorrect architecture')\n",
    "\n",
    "    # Define the optimizer and learning rate\n",
    "    optimizer = obtain_optimizer(config, net)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "        step_size=config['decay_step'],\n",
    "        gamma=config['decay_gamma'])\n",
    "\n",
    "    # Use pretrained model\n",
    "    if config['pretrain_snapshot']:\n",
    "        logging.info('Load pretrained weights from {}'.format(config['pretrain_snapshot']))\n",
    "        state_dict = torch.load(config['pretrain_snapshot'])\n",
    "        net.load_state_dict(state_dict)\n",
    "\n",
    "    # Use all 3 scales for computing YOLO loss\n",
    "    YOLO_losses = []\n",
    "    val_YOLO_losses = []\n",
    "    test_YOLO_losses = []\n",
    "    for i in range(3):\n",
    "        YOLO_losses.append(YoloLoss(config['classes'], (config['img_w'], config['img_h']), config['anchors'][i]))\n",
    "        val_YOLO_losses.append(YoloLoss(config['classes'], (config['img_w'], config['img_h']), config['anchors'][i]))\n",
    "        test_YOLO_losses.append(YoloLoss(config['classes'], (config['img_w'], config['img_h']), config['anchors'][i]))\n",
    "\n",
    "    # Check if your system supports CUDA\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Setup GPU optimization if CUDA is supported\n",
    "    if use_cuda:\n",
    "        computing_device = torch.device(\"cuda\")\n",
    "        extras = {\"num_workers\": 3, \"pin_memory\": True}\n",
    "        print(\"CUDA is supported\")\n",
    "    else: # Otherwise, train on the CPU\n",
    "        computing_device = torch.device(\"cpu\")\n",
    "        extras = False\n",
    "        print(\"CUDA NOT supported\")\n",
    "    \n",
    "    # Load in data \n",
    "    root_dir = os.getcwd()\n",
    "    train_loader, val_loader, test_loader = create_split_loaders(config['batch_size'])\n",
    "    \n",
    "    # Instantiate model to run on the GPU or CPU based on CUDA support\n",
    "    net = net.to(computing_device)\n",
    "    print(\"Model on CUDA?\", next(net.parameters()).is_cuda)\n",
    "    \n",
    "    # Begin training loop\n",
    "    print(\"Start training:\")\n",
    "    for epoch in range(config['epochs']):\n",
    "        N_minibatch_loss = 0.0\n",
    "        for minibatch, samples in enumerate(train_loader):\n",
    "            images, labels = samples[\"image\"], samples[\"label\"]\n",
    "            #print(labels)\n",
    "            start_time = time.time()\n",
    "            config['global_step'] += 1\n",
    "\n",
    "            # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "            images = images.to(computing_device)\n",
    "            #labels = labels.to(computing_device)\n",
    "\n",
    "            # Forward and backward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss_names = [\"total_loss\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"cls\"]\n",
    "            losses = []\n",
    "            for z in range(len(loss_names)):\n",
    "                losses.append([])\n",
    "            for i in range(3):\n",
    "                loss_item = YOLO_losses[i](outputs[i], labels)\n",
    "                for j, l in enumerate(loss_item):\n",
    "                    losses[j].append(l)\n",
    "            losses = [sum(l) for l in losses]\n",
    "            loss = losses[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add this iteration's loss to the total_loss\n",
    "            total_loss[epoch][minibatch] = loss.item()\n",
    "            N_minibatch_loss += loss\n",
    "\n",
    "            if minibatch > 0 and minibatch % 10 == 0:\n",
    "                _loss = loss.item()\n",
    "                N_minibatch_loss /= 10\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                logging.info('Epoch [%.3d] Minibatch = %d Loss = %.2f lr = %.5f '%\n",
    "                    (epoch, minibatch, N_minibatch_loss, lr))\n",
    "                print('Epoch [%.3d] Minibatch = %d Loss = %.2f lr = %.5f '%\n",
    "                    (epoch, minibatch, N_minibatch_loss, lr))\n",
    "                \n",
    "                # Add the averaged loss over N minibatches and reset the counter\n",
    "                avg_minibatch_loss.append(N_minibatch_loss.item())\n",
    "                N_minibatch_loss = 0.0\n",
    "            \n",
    "                for i, name in enumerate(loss_names):\n",
    "                    value = _loss if i == 0 else losses[i]\n",
    "\n",
    "            if epoch==0 and minibatch % 205 == 0:\n",
    "                save_checkpoint(net.state_dict(), config)\n",
    "                \n",
    "                # Implement cross-validation\n",
    "                val_loss = 0\n",
    "                sum_val_loss = 0\n",
    "                validation_outputs = []\n",
    "                validation_labels = []\n",
    "                with torch.no_grad():\n",
    "                    for valid_batch_count, samples in enumerate(val_loader):\n",
    "                        val_images, val_labels = samples[\"image\"], samples[\"label\"]\n",
    "                        val_images = val_images.to(computing_device)\n",
    "                        val_outputs = net(val_images)\n",
    "                        val_losses = []\n",
    "                        for z in range(len(loss_names)):\n",
    "                            val_losses.append([])\n",
    "                        for i in range(3):\n",
    "                            val_loss_item = val_YOLO_losses[i](val_outputs[i], val_labels)\n",
    "                            for j, l in enumerate(val_loss_item):\n",
    "                                val_losses[j].append(l)\n",
    "                        val_losses = [sum(l) for l in val_losses]\n",
    "                        val_loss = val_losses[0]\n",
    "                        sum_val_loss += val_loss\n",
    "                        print(\"\\tvalid_batch_count: \", valid_batch_count)\n",
    "\n",
    "                    sum_val_loss /= len(val_loader)\n",
    "                    print(\"avg validation loss: \", sum_val_loss)\n",
    "                    avg_valid_loss.append(sum_val_loss.item())\n",
    "                    \n",
    "\n",
    "            if epoch > 0 and minibatch > 0 and minibatch % 205 == 0:\n",
    "                save_checkpoint(net.state_dict(), config)\n",
    "                \n",
    "                # Implement cross-validation\n",
    "                val_loss = 0\n",
    "                sum_val_loss = 0\n",
    "                validation_outputs = []\n",
    "                validation_labels = []\n",
    "                with torch.no_grad():\n",
    "                    for valid_batch_count, samples in enumerate(val_loader):\n",
    "                        val_images, val_labels = samples[\"image\"], samples[\"label\"]\n",
    "                        val_images = val_images.to(computing_device)\n",
    "                        val_outputs = net(val_images)\n",
    "                        val_losses = []\n",
    "                        for z in range(len(loss_names)):\n",
    "                            val_losses.append([])\n",
    "                        for i in range(3):\n",
    "                            val_loss_item = val_YOLO_losses[i](val_outputs[i], val_labels)\n",
    "                            for j, l in enumerate(val_loss_item):\n",
    "                                val_losses[j].append(l)\n",
    "                        val_losses = [sum(l) for l in val_losses]\n",
    "                        val_loss = val_losses[0]\n",
    "                        sum_val_loss += val_loss\n",
    "                        print(\"\\tvalid_batch_count: \", valid_batch_count)\n",
    "\n",
    "                    sum_val_loss /= len(val_loader)\n",
    "                    print(\"avg validation loss: \", sum_val_loss)\n",
    "                    avg_valid_loss.append(sum_val_loss.item())\n",
    "                    \n",
    "        lr_scheduler.step()\n",
    "\n",
    "    save_checkpoint(net.state_dict(), config)\n",
    "    print('Training Complete')\n",
    "    \n",
    "    test_loss = 0\n",
    "    sum_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for test_batch_count, samples in enumerate(test_loader):\n",
    "            test_images, test_labels = samples[\"image\"], samples[\"label\"]\n",
    "            print(\"\\ttest_batch_count: \", test_batch_count)\n",
    "            test_images= test_images.to(computing_device)\n",
    "            test_outputs = net(test_images)\n",
    "            test_losses = []\n",
    "            for z in range(len(loss_names)):\n",
    "                test_losses.append([])\n",
    "            for i in range(3):\n",
    "                test_loss_item = test_YOLO_losses[i](test_outputs[i], test_labels)\n",
    "                for j, l in enumerate(val_loss_item):\n",
    "                    test_losses[j].append(l)\n",
    "            test_losses = [sum(l) for l in test_losses]\n",
    "            test_loss = test_losses[0]\n",
    "            sum_test_loss += test_loss\n",
    "\n",
    "        sum_test_loss /= len(test_loader)\n",
    "        print(\"avg test loss: \", sum_test_loss)\n",
    "        avg_test_loss[epoch] = test_loss.item()\n",
    "    \n",
    "    return avg_minibatch_loss, avg_valid_loss\n",
    "\n",
    "    \n",
    "def save_checkpoint(state_dict, config, evaluate_func=None):\n",
    "        \n",
    "    checkpoint_path = os.path.join(config[\"sub_working_dir\"], \"model.pth\")\n",
    "    torch.save(state_dict, checkpoint_path)\n",
    "    logging.info(\"Model checkpoint saved to %s\" % checkpoint_path)\n",
    "\n",
    "\n",
    "def obtain_optimizer(config, net):\n",
    "    optimizer = None\n",
    "\n",
    "    # Assign different learning rate for each layer\n",
    "    params = None\n",
    "    base_parameters = list(\n",
    "        map(id, net.backbone.parameters())\n",
    "    )\n",
    "    logits_parameters = filter(lambda p: id(p) not in base_parameters, net.parameters())\n",
    "\n",
    "    if not config['freeze_backbone']:\n",
    "        parameters = [\n",
    "            {\"params\": logits_parameters, \"lr\": config['other_lr']},\n",
    "            {\"params\": net.backbone.parameters(), \"lr\": config['backbone_lr']},\n",
    "        ]\n",
    "    else:\n",
    "        print(\"Freezing backbone parameters\")\n",
    "        for p in net.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        parameters = [\n",
    "            {\"params\": logits_parameters, \"lr\": config['other_lr']},\n",
    "        ]\n",
    "\n",
    "    # Initialize optimizer class\n",
    "    if config['optimizer_type'] == \"adam\":\n",
    "        optimizer = optim.Adam(params, weight_decay=config['optimizer_weight_decay'])\n",
    "    elif config['optimizer_type'] == \"amsgrad\":\n",
    "        optimizer = optim.Adam(params, weight_decay=config['optimizer_weight_decay'], amsgrad=True)\n",
    "    elif config['optimizer_type'] == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(params, weight_decay=config['optimizer_weight_decay'])\n",
    "    else:\n",
    "        optimizer = optim.SGD(parameters, momentum=0.9, weight_decay=config['optimizer_weight_decay'],\n",
    "                              nesterov=(config['optimizer_type'] == \"nesterov\"))\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(architecture='Baseline', output_file=None):\n",
    "    \n",
    "    if output_file is None:\n",
    "        output_file = \"_\".join([str(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))]) + \".json\"\n",
    "        print(output_file)\n",
    "        \n",
    "    log['data'] = {\n",
    "        'total_loss': total_loss,\n",
    "        'avg_minibatch_loss': avg_minibatch_loss,\n",
    "        'avg_valid_loss': avg_valid_loss,\n",
    "        'validation_metrics': _validation_metrics,\n",
    "        'test_outputs': tot_test_outputs,\n",
    "        'test_labels': tot_test_labels\n",
    "    }\n",
    "        \n",
    "    with open(\"logs/\" + output_file, 'w') as f:\n",
    "        json.dump(log, f, indent=4)\n",
    "    \n",
    "    # Initialize hyperparameters/variables\n",
    "    config = {}\n",
    "    config['backbone_name'] = \"darknet_53\"\n",
    "    config['backbone_pretrained'] = \"./darknet53_weights_pytorch.pth\" # set empty to disable\n",
    "    \n",
    "    config['anchors'] = [[[116, 90], [156, 198], [373, 326]],\n",
    "                                [[30, 61], [62, 45], [59, 119]],\n",
    "                                [[10, 13], [16, 30], [33, 23]]]\n",
    "    config['classes'] = 20\n",
    "    \n",
    "    config['backbone_lr'] = 0.001\n",
    "    config['other_lr'] = 0.01\n",
    "    config['freeze_backbone'] = False   #  freeze backbone wegiths to finetune\n",
    "    config['decay_gamma'] = 0.5\n",
    "    config['decay_step'] = 6         #  decay lr in every ? epochs\n",
    "    \n",
    "    config['optimizer_type'] = \"sgd\"\n",
    "    config['optimizer_weight_decay'] = 4e-05\n",
    "    \n",
    "    if architecture == 'Baseline':\n",
    "        config['batch_size'] = 20  # Number of training samples per batch to be passed to network\n",
    "    elif architecture == 'DeformConv':\n",
    "        config['batch_size'] = 10\n",
    "        \n",
    "    config['epochs'] = 10  # Number of epochs to train the model\n",
    "    config['img_h'] = config['img_w'] = 416\n",
    "    config['seed'] = np.random.seed()\n",
    "    config['working_dir'] = \"./states\"     #  replace with your working dir\n",
    "    \n",
    "    def get_latest_states(dirpath):\n",
    "        \"\"\"\n",
    "        Get the latest image file in the given directory\n",
    "        \"\"\"\n",
    "        # get filepaths of all files and dirs in the given dir\n",
    "        valid_files = [os.path.join(dirpath, filename) for filename in os.listdir(dirpath)]\n",
    "\n",
    "        return max(valid_files, key=os.path.getmtime)\n",
    "   \n",
    "    \n",
    "    # Create sub_working_dir\n",
    "    sub_working_dir = '{}/{}_{}'.format(\n",
    "        config['working_dir'], time.strftime(\"%Y%m%d%H%M%S\", time.localtime()), architecture)\n",
    "    if not os.path.exists(sub_working_dir):\n",
    "        os.makedirs(sub_working_dir)\n",
    "    config[\"sub_working_dir\"] = sub_working_dir\n",
    "    logging.info(\"sub working dir: %s\" % sub_working_dir)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(sub_working_dir, \"model.pth\")):\n",
    "        config['pretrain_snapshot'] = \"\"\n",
    "    else:\n",
    "        config['pretrain_snapshot'] = os.path.join(get_latest_states(config['working_dir']), \"model.pth\")  # load checkpoint\n",
    "\n",
    "    # Start training\n",
    "    train_loss, valid_loss = train(config, architecture)\n",
    "    \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_loss, valid_loss = main(architecture='Baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot( X, Y, title, xlabel, ylabel, labels ):\n",
    "    for index, (x,y) in enumerate( zip( X, Y ) ):\n",
    "        plt.plot( x, y, label=labels[index] )\n",
    "    plt.legend(loc='best'); plt.title(title); plt.ylabel(ylabel); plt.xlabel(xlabel); plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_loss, valid_loss\n",
    "X, Y = [ range(0,10*len(train),10), range(0,205*len(val),205) ], [ train, val ]\n",
    "title, xlabel, ylabel = 'Loss vs Minibatches', 'Minibatches', 'Loss'\n",
    "labels = ['train', 'val' ]\n",
    "plot( X, Y, title, xlabel, ylabel, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
